{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an LSTM to demonstrate the concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradiants caused by repeated matrix multiplication. LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism as follows.\n",
    "\n",
    "Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
    "\n",
    "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $i\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "i = \\sigma(a_i) \\hspace{2pc} \\newline\n",
    "f = \\sigma(a_f) \\hspace{2pc} \\newline\n",
    "o = \\sigma(a_o) \\hspace{2pc} \\newline\n",
    "g = \\tanh(a_g)  \\hspace{2pc} \\newline\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "This is an image of an LSTM cell with all the important equations\n",
    "\n",
    "\n",
    "<img src=\"images/LSTM2.png\"  align=\"center\">\n",
    "\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
    "\n",
    "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc} \\newline\n",
    "h_t = o\\odot\\tanh(c_t) \\hspace{4pc} \\newline\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\odot$ is the elementwise product of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "\n",
    "def sigmoid(x):\n",
    "  \"\"\"\n",
    "  A numerically stable version of the logistic sigmoid activation function.\n",
    "  \"\"\"\n",
    "  pos_mask = (x >= 0)\n",
    "  neg_mask = (x < 0)\n",
    "  z = np.zeros_like(x)\n",
    "  z[pos_mask] = np.exp(-x[pos_mask])\n",
    "  z[neg_mask] = np.exp(x[neg_mask])\n",
    "  top = np.ones_like(x)\n",
    "  top[neg_mask] = z[neg_mask]\n",
    "  return top / (1 + z)\n",
    "\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):\n",
    "  \"\"\"\n",
    "  Forward pass for a single timestep of an LSTM.\n",
    "  \n",
    "  The input data has dimension D, the hidden state has dimension H, and we use\n",
    "  a minibatch size of N.\n",
    "  \n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, D)\n",
    "  - prev_h: Previous hidden state, of shape (N, H)\n",
    "  - prev_c: previous cell state, of shape (N, H)\n",
    "  - Wx: Input-to-hidden weights, of shape (D, 4H)\n",
    "  - Wh: Hidden-to-hidden weights, of shape (H, 4H)\n",
    "  - b: Biases, of shape (4H,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - next_h: Next hidden state, of shape (N, H)\n",
    "  - next_c: Next cell state, of shape (N, H)\n",
    "  - cache: Tuple of values needed for backward pass.\n",
    "  \"\"\"\n",
    "  next_h, next_c, cache = None, None, None\n",
    "  N, H = prev_h.shape\n",
    "  #############################################################################\n",
    "  # Implementing the forward pass for a single timestep of an LSTM.        #\n",
    "  #############################################################################\n",
    "  activation = x.dot(Wx) + prev_h.dot(Wh) + b\n",
    "  # Splitting in four groups\n",
    "  input,forget,output,block_input = sigmoid(activation[:,0:H]),sigmoid(activation[:,H:2*H]),sigmoid(activation[:,2*H:3*H]),np.tanh(activation[:,3*H:])\n",
    "    \n",
    "  next_c = forget * prev_c + input * block_input\n",
    "  next_h = output * np.tanh(next_c)\n",
    "  \n",
    "\n",
    "  cache = (x, prev_h, prev_c, next_h, next_c, Wx, Wh, b, activation, input,\n",
    "            forget, output, block_input)\n",
    "  return next_h, next_c, cache\n",
    "\n",
    "\n",
    "def lstm_step_backward(dnext_h, dnext_c, cache):\n",
    "  \"\"\"\n",
    "  Backward pass for a single timestep of an LSTM.\n",
    "  \n",
    "  Inputs:\n",
    "  - dnext_h: Gradients of next hidden state, of shape (N, H)\n",
    "  - dnext_c: Gradients of next cell state, of shape (N, H)\n",
    "  - cache: Values from the forward pass\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient of input data, of shape (N, D)\n",
    "  - dprev_h: Gradient of previous hidden state, of shape (N, H)\n",
    "  - dprev_c: Gradient of previous cell state, of shape (N, H)\n",
    "  - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)\n",
    "  - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)\n",
    "  - db: Gradient of biases, of shape (4H,)\n",
    "  \"\"\"\n",
    "  N, H = dnext_h.shape\n",
    "  (x, prev_h, prev_c, next_h, next_c, Wx, Wh,\n",
    "  b, activation, input, forget, output, block_input) = cache\n",
    "  #############################################################################\n",
    "  # Implementing the backward pass for a single timestep of an LSTM.       #\n",
    "  #############################################################################\n",
    "  dnext_c += dnext_h*output*(1-np.tanh(next_c)**2)\n",
    "  dprev_c = dnext_c*forget\n",
    "  dinput_gate = dnext_c*block_input\n",
    "  dblock_input = dnext_c*input\n",
    "  dforget_gate = dnext_c*prev_c\n",
    "  doutput_gate = dnext_h*np.tanh(next_c)\n",
    "  \n",
    "  # d_sigmoid = sigmoid(1-sigmoid)\n",
    "  # d_tanh = (1-(tanh)^2)\n",
    "  dactivation = np.zeros((N, 4*H))\n",
    "  dactivation[:,0:H] = dinput_gate*input*(1-input)\n",
    "  dactivation[:,H:2*H] = dforget_gate*forget*(1-forget)\n",
    "  dactivation[:,2*H:3*H] = doutput_gate*output*(1-output)\n",
    "  dactivation[:,3*H:] = dblock_input*(1-block_input**2)\n",
    "\n",
    "  dx = np.dot(dactivation,Wx.T)\n",
    "  dWx = np.dot(x.T, dactivation)\n",
    "  dWh = np.dot(prev_h.T, dactivation)\n",
    "  dprev_h = np.dot(dactivation, Wh.T)\n",
    "  db = np.sum(dactivation, axis=0)\n",
    "\n",
    "  return dx, dprev_h, dprev_c, dWx, dWh, db\n",
    "\n",
    "\n",
    "def lstm_forward(x, h0, Wx, Wh, b):\n",
    "  \"\"\"\n",
    "  Forward pass for an LSTM over an entire sequence of data. We assume an input\n",
    "  sequence composed of T vectors, each of dimension D. The LSTM uses a hidden\n",
    "  size of H, and we work over a minibatch containing N sequences. After running\n",
    "  the LSTM forward, we return the hidden states for all timesteps.\n",
    "  \n",
    "  Note that the initial cell state is passed as input, but the initial cell\n",
    "  state is set to zero. Also note that the cell state is not returned; it is\n",
    "  an internal variable to the LSTM and is not accessed from outside.\n",
    "  \n",
    "  Inputs:\n",
    "  - x: Input data of shape (N, T, D)\n",
    "  - h0: Initial hidden state of shape (N, H)\n",
    "  - Wx: Weights for input-to-hidden connections, of shape (D, 4H)\n",
    "  - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)\n",
    "  - b: Biases of shape (4H,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)\n",
    "  - cache: Values needed for the backward pass.\n",
    "  \"\"\"\n",
    "  \n",
    "  N, T, D = x.shape\n",
    "  H = Wh.shape[0]\n",
    "  h, seq_cache = np.zeros((N, T+1, H)), {} # initializing with T+1 to consider the prev_h\n",
    "  #############################################################################\n",
    "  # Implement the forward pass for an LSTM over an entire timeseries.   \n",
    "  # Using the lstm_step_forward      \n",
    "  #############################################################################\n",
    "  # Initial state\n",
    "  h[:,0,:] = h0\n",
    "  prev_c = np.zeros_like(h0)\n",
    "\n",
    "  for i in range(T):\n",
    "      h[:,i+1,:], prev_c, seq_cache[i] = lstm_step_forward(x[:,i,:], h[:,i,:], prev_c, Wx, Wh, b)\n",
    "  # Rest of it from 1 to T+1\n",
    "  h = h[:,1:,:]\n",
    "    \n",
    "  return h, seq_cache\n",
    "\n",
    "\n",
    "def lstm_backward(dh, cache):\n",
    "  \"\"\"\n",
    "  Backward pass for an LSTM over an entire sequence of data.]\n",
    "  \n",
    "  Inputs:\n",
    "  - dh: Upstream gradients of hidden states, of shape (N, T, H)\n",
    "  - cache: Values from the forward pass\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient of input data of shape (N, T, D)\n",
    "  - dh0: Gradient of initial hidden state of shape (N, H)\n",
    "  - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)\n",
    "  - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)\n",
    "  - db: Gradient of biases, of shape (4H,)\n",
    "  \"\"\"\n",
    "  N, T, H = dh.shape\n",
    "  _, D = cache[0][0].shape  \n",
    "  dx = np.zeros((N, T, D))\n",
    "  dh0 = np.zeros((N, H))\n",
    "\n",
    "  dWx = np.zeros((D, 4*H))\n",
    "  dWh = np.zeros((H, 4*H))\n",
    "  db = np.zeros((4 * H))\n",
    "    \n",
    "  \n",
    "  #############################################################################\n",
    "  # Implement the backward pass for an LSTM over an entire timeseries.  #\n",
    "  # Using the lstm_step_backward function \n",
    "  #############################################################################\n",
    "  dprev = np.zeros((N, H))\n",
    "  dprev_h = np.zeros((N, H))\n",
    "  dprev_c = np.zeros((N, H))\n",
    "    \n",
    "  for i in reversed(range(T)):\n",
    "    dh_current = dh[:,i,:] + dprev\n",
    "    dx_t, dprev, dprev_c, dWx_t, dWh_t, db_t = lstm_step_backward(dh_current, dprev_c, cache[i])\n",
    "    dx[:,i,:] += dx_t\n",
    "    db += db_t\n",
    "    dWx += dWx_t\n",
    "    dWh += dWh_t\n",
    "\n",
    "  dh0 += dprev\n",
    "  return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: step forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  5.70541319671e-09\n",
      "next_c error:  5.81431230888e-09\n"
     ]
    }
   ],
   "source": [
    "from grad_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "\n",
    "\n",
    "N, D, H = 3, 4, 5\n",
    "x = np.linspace(-0.4, 1.2, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.3, 0.7, num=N*H).reshape(N, H)\n",
    "prev_c = np.linspace(-0.4, 0.9, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-2.1, 1.3, num=4*D*H).reshape(D, 4 * H)\n",
    "Wh = np.linspace(-0.7, 2.2, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.3, 0.7, num=4*H)\n",
    "\n",
    "\n",
    "next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)\n",
    "\n",
    "expected_next_h = np.asarray([\n",
    "    [ 0.24635157,  0.28610883,  0.32240467,  0.35525807,  0.38474904],\n",
    "    [ 0.49223563,  0.55611431,  0.61507696,  0.66844003,  0.7159181 ],\n",
    "    [ 0.56735664,  0.66310127,  0.74419266,  0.80889665,  0.858299  ]])\n",
    "expected_next_c = np.asarray([\n",
    "    [ 0.32986176,  0.39145139,  0.451556,    0.51014116,  0.56717407],\n",
    "    [ 0.66382255,  0.76674007,  0.87195994,  0.97902709,  1.08751345],\n",
    "    [ 0.74192008,  0.90592151,  1.07717006,  1.25120233,  1.42395676]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))\n",
    "print('next_c error: ', rel_error(expected_next_c, next_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: step backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  8.22384498463e-11\n",
      "dh error:  3.60709005556e-09\n",
      "dc error:  5.22982709789e-09\n",
      "dWx error:  1.08644815961e-06\n",
      "dWh error:  3.55458024424e-06\n",
      "db error:  2.20473550598e-09\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "prev_h = np.random.randn(N, H)\n",
    "prev_c = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)\n",
    "\n",
    "dnext_h = np.random.randn(*next_h.shape)\n",
    "dnext_c = np.random.randn(*next_c.shape)\n",
    "\n",
    "fx_h = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fh_h = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fc_h = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fWx_h = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fWh_h = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fb_h = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "\n",
    "fx_c = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fh_c = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fc_c = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fWx_c = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fWh_c = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fb_c = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "\n",
    "num_grad = eval_numerical_gradient_array\n",
    "\n",
    "dx_num = num_grad(fx_h, x, dnext_h) + num_grad(fx_c, x, dnext_c)\n",
    "dh_num = num_grad(fh_h, prev_h, dnext_h) + num_grad(fh_c, prev_h, dnext_c)\n",
    "dc_num = num_grad(fc_h, prev_c, dnext_h) + num_grad(fc_c, prev_c, dnext_c)\n",
    "dWx_num = num_grad(fWx_h, Wx, dnext_h) + num_grad(fWx_c, Wx, dnext_c)\n",
    "dWh_num = num_grad(fWh_h, Wh, dnext_h) + num_grad(fWh_c, Wh, dnext_c)\n",
    "db_num = num_grad(fb_h, b, dnext_h) + num_grad(fb_c, b, dnext_c)\n",
    "\n",
    "dx, dh, dc, dWx, dWh, db = lstm_step_backward(dnext_h, dnext_c, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh error: ', rel_error(dh_num, dh))\n",
    "print('dc error: ', rel_error(dc_num, dc))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h error:  8.61053745211e-08\n"
     ]
    }
   ],
   "source": [
    "N, D, H, T = 2, 5, 4, 3\n",
    "x = np.linspace(-0.4, 0.6, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.4, 0.8, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.9, num=4*D*H).reshape(D, 4 * H)\n",
    "Wh = np.linspace(-0.3, 0.6, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.2, 0.7, num=4*H)\n",
    "\n",
    "h, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "expected_h = np.asarray([\n",
    " [[ 0.01764008,  0.01823233,  0.01882671,  0.0194232 ],\n",
    "  [ 0.11287491,  0.12146228,  0.13018446,  0.13902939],\n",
    "  [ 0.31358768,  0.33338627,  0.35304453,  0.37250975]],\n",
    " [[ 0.45767879,  0.4761092,   0.4936887,   0.51041945],\n",
    "  [ 0.6704845,   0.69350089,  0.71486014,  0.7346449 ],\n",
    "  [ 0.81733511,  0.83677871,  0.85403753,  0.86935314]]])\n",
    "\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  4.97098739471e-08\n",
      "dh0 error:  4.22335407992e-10\n",
      "dWx error:  6.70466622004e-09\n",
      "dWh error:  1.00810215428e-08\n",
      "db error:  2.60904317045e-09\n"
     ]
    }
   ],
   "source": [
    "N, D, T, H = 2, 3, 10, 6\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "out, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This implementation of LSTM seems to be working perfectly with minimal error in gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
