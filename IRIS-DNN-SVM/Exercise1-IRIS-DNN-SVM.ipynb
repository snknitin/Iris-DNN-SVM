{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logo.png\" style=\"height:75px;width:120px;\" align=\"left\">\n",
    "\n",
    "<br><br><h1>Example of using TensorFlow to build a Classifier for Iris Data</h1>\n",
    "<img src=\"images/Iris-Example-1.png\">\n",
    "<img src=\"images/Iris-Example-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logo.png\" style=\"height:75px;width:120px;\" align=\"left\">\n",
    "<h2>Example of a Deep Neural Network (DNN) to classify Iris Petals into three Classes [0,1,2] </h2>\n",
    "<h3> The code below is adapted from the TensorFlow example, with modifications </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "def iris_classifier_dnn(hidden_units=None, training_steps=2000, \n",
    "                    model_dir=\"/tmp/iris_model\",\n",
    "                    delete_old_model=False\n",
    "                   ):\n",
    "    \n",
    "  # Set the model type to be SVM-{kernel}\n",
    "  modelType = \"DNN\"\n",
    "    \n",
    "  # 0 represents setosa 1 represents versicolor 2 represents virginica\n",
    "  iris_classes = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "    \n",
    "  # If the training and test sets aren't stored locally, download them.\n",
    "  if not os.path.exists(IRIS_TRAINING):\n",
    "    raw = urlopen(IRIS_TRAINING_URL).read()\n",
    "    with open(IRIS_TRAINING, \"wb\") as f:\n",
    "      f.write(raw)\n",
    "\n",
    "  if not os.path.exists(IRIS_TEST):\n",
    "    raw = urlopen(IRIS_TEST_URL).read()\n",
    "    with open(IRIS_TEST, \"wb\") as f:\n",
    "      f.write(raw)\n",
    "\n",
    "  if delete_old_model:\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "        print(\"Deleting old model {0}\\n\" .format(model_dir))\n",
    "  # Load datasets.\n",
    "  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TRAINING,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "  test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TEST,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "\n",
    "  # Specify that all features have real-value data\n",
    "  feature_columns = [tf.feature_column.numeric_column(\"x\", \n",
    "                                                      shape=[training_set.data.shape[1]])]\n",
    "\n",
    "  print(\"Featured Columns {0}\\n\" .format(feature_columns))                                                    \n",
    "  # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "  print(\"Building DNN with {0} hidden units\\n\" .format(hidden_units))\n",
    "  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                          hidden_units=hidden_units,\n",
    "                                          n_classes=3,\n",
    "                                          model_dir=\"/tmp/iris_model\")\n",
    "  # Define the training inputs\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(training_set.data)},\n",
    "      y=np.array(training_set.target),\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "  # Train model.\n",
    "  classifier.train(input_fn=train_input_fn, steps=training_steps)\n",
    "\n",
    "  # Define the test inputs\n",
    "  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(test_set.data)},\n",
    "      y=np.array(test_set.target),\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "  # Evaluate accuracy.\n",
    "  accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "\n",
    "  print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
    "\n",
    "  # Classify two new flower samples.\n",
    "  new_samples = np.array(\n",
    "      [[6.4, 3.2, 4.5, 1.5],\n",
    "       [5.8, 3.1, 5.0, 1.7],\n",
    "       [6.4 ,2.8, 5.6 ,2.2],\n",
    "       [5.0 , 2.3 ,3.3 ,1.0 ],\n",
    "       [4.9 ,2.5 ,4.5 ,1.7 ],\n",
    "       [4.9 ,3.1 ,1.5 ,0.1 ],\n",
    "       [5.7,3.8 ,1.7 ,0.3]], dtype=np.float32)\n",
    "    \n",
    "     \n",
    "  predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": new_samples},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "  predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "  \n",
    "  predicted_classes = [p[\"class_ids\"] for p in predictions]\n",
    "  # Store probabilities for each prediction  \n",
    "  probs = [p[\"probabilities\"] for p in predictions]\n",
    "  probabilities = []\n",
    "  predicted_class_names = []\n",
    "  for i in range(len(predicted_classes)):\n",
    "    predicted_classes[i] = int(predicted_classes[i])\n",
    "    predicted_class_names.append(iris_classes[int(predicted_classes[i])])\n",
    "    pr = probs[i]\n",
    "    probabilities.append(pr[int(predicted_classes[i])]*100.0)\n",
    "\n",
    "  print(\n",
    "      \"New Samples, Class Predictions:    {}\\n\"\n",
    "      .format(predicted_classes))\n",
    "  result= {\"training-size\" :  training_set.data.shape,\n",
    "           \"test-size\" : test_set.data.shape,\n",
    "           \"train-steps\" : training_steps,\n",
    "           \"model_dir\"  : model_dir,\n",
    "           \"model-type\" : modelType,\n",
    "           \"hidden-units\" : str(hidden_units),\n",
    "           \"accuracy\" : accuracy_score, \n",
    "           \"prediction\": predicted_classes,\n",
    "           \"prediction-names\": predicted_class_names,\n",
    "           \"probabilities\" : probabilities}\n",
    "\n",
    "  return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logo.png\" style=\"height:75px;width:120px;\" align=\"left\">\n",
    "<h2> Run the Code above and Below: This will create a DNN and run predictions on the Iris model data</h2>\n",
    "    <ul><li> You can play with the hidden_units and training steps to see if this makes a difference to accuracy and predictions\n",
    "    </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.3.0\n",
      "\n",
      "Deleting old model /tmp/iris_model\n",
      "\n",
      "Featured Columns [_NumericColumn(key='x', shape=(4,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
      "\n",
      "Building DNN with [10, 30, 10] hidden units\n",
      "\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_tf_random_seed': 1, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/iris_model', '_log_step_count_steps': 100, '_session_config': None}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 307.948, step = 1\n",
      "INFO:tensorflow:global_step/sec: 814.95\n",
      "INFO:tensorflow:loss = 20.1675, step = 101 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 839.017\n",
      "INFO:tensorflow:loss = 9.77627, step = 201 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 806.892\n",
      "INFO:tensorflow:loss = 12.2683, step = 301 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 838.868\n",
      "INFO:tensorflow:loss = 8.45627, step = 401 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 721.556\n",
      "INFO:tensorflow:loss = 21.4563, step = 501 (0.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 824.412\n",
      "INFO:tensorflow:loss = 11.7478, step = 601 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 856.052\n",
      "INFO:tensorflow:loss = 11.0096, step = 701 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 880.531\n",
      "INFO:tensorflow:loss = 5.25654, step = 801 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.858\n",
      "INFO:tensorflow:loss = 3.05667, step = 901 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 841.817\n",
      "INFO:tensorflow:loss = 4.76032, step = 1001 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 835.455\n",
      "INFO:tensorflow:loss = 13.3801, step = 1101 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 793.828\n",
      "INFO:tensorflow:loss = 11.1625, step = 1201 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 852.295\n",
      "INFO:tensorflow:loss = 9.5361, step = 1301 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 831.171\n",
      "INFO:tensorflow:loss = 4.91319, step = 1401 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 724.111\n",
      "INFO:tensorflow:loss = 3.10998, step = 1501 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 850.549\n",
      "INFO:tensorflow:loss = 6.90044, step = 1601 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 864.05\n",
      "INFO:tensorflow:loss = 5.69742, step = 1701 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 818.466\n",
      "INFO:tensorflow:loss = 12.2282, step = 1801 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.182\n",
      "INFO:tensorflow:loss = 7.05295, step = 1901 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 817.724\n",
      "INFO:tensorflow:loss = 7.51846, step = 2001 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 802.248\n",
      "INFO:tensorflow:loss = 5.50152, step = 2101 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 833.99\n",
      "INFO:tensorflow:loss = 6.35155, step = 2201 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 820.726\n",
      "INFO:tensorflow:loss = 10.6389, step = 2301 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 854.235\n",
      "INFO:tensorflow:loss = 2.75188, step = 2401 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 811.538\n",
      "INFO:tensorflow:loss = 1.67039, step = 2501 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 831.281\n",
      "INFO:tensorflow:loss = 10.3763, step = 2601 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 826.111\n",
      "INFO:tensorflow:loss = 11.9559, step = 2701 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 832.046\n",
      "INFO:tensorflow:loss = 9.38388, step = 2801 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 822.587\n",
      "INFO:tensorflow:loss = 5.67132, step = 2901 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 808.492\n",
      "INFO:tensorflow:loss = 7.43904, step = 3001 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 792.688\n",
      "INFO:tensorflow:loss = 6.60916, step = 3101 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 817.852\n",
      "INFO:tensorflow:loss = 8.14018, step = 3201 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 813.261\n",
      "INFO:tensorflow:loss = 9.72542, step = 3301 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 841.132\n",
      "INFO:tensorflow:loss = 4.50017, step = 3401 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 852.457\n",
      "INFO:tensorflow:loss = 2.41342, step = 3501 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 851.032\n",
      "INFO:tensorflow:loss = 9.18219, step = 3601 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 838.644\n",
      "INFO:tensorflow:loss = 5.54573, step = 3701 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 837.477\n",
      "INFO:tensorflow:loss = 12.0299, step = 3801 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 856.276\n",
      "INFO:tensorflow:loss = 1.97804, step = 3901 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 827.7\n",
      "INFO:tensorflow:loss = 6.20923, step = 4001 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 837.728\n",
      "INFO:tensorflow:loss = 5.10362, step = 4101 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 843.77\n",
      "INFO:tensorflow:loss = 3.2605, step = 4201 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 740.302\n",
      "INFO:tensorflow:loss = 2.62431, step = 4301 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 862.458\n",
      "INFO:tensorflow:loss = 2.53673, step = 4401 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 836.549\n",
      "INFO:tensorflow:loss = 8.29795, step = 4501 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 820.212\n",
      "INFO:tensorflow:loss = 5.22555, step = 4601 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 754.586\n",
      "INFO:tensorflow:loss = 4.07201, step = 4701 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 795.542\n",
      "INFO:tensorflow:loss = 12.1887, step = 4801 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 890.389\n",
      "INFO:tensorflow:loss = 4.3911, step = 4901 (0.112 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.06634.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-25-10:35:24\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-25-10:35:24\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.966667, average_loss = 0.0622638, global_step = 5000, loss = 1.86791\n",
      "\n",
      "Test Accuracy: 0.966667\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-5000\n",
      "New Samples, Class Predictions:    [1, 2, 2, 1, 2, 0, 0]\n",
      "\n",
      "Result: {'model_dir': '/tmp/iris_model', 'test-size': (30, 4), 'probabilities': [99.989831447601318, 76.146531105041504, 99.976509809494019, 99.865388870239258, 99.187743663787842, 99.987530708312988, 99.989604949951172], 'prediction-names': ['versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa'], 'prediction': [1, 2, 2, 1, 2, 0, 0], 'model-type': 'DNN', 'hidden-units': '[10, 30, 10]', 'accuracy': 0.96666664, 'training-size': (120, 4), 'train-steps': 5000}\n",
      "\n",
      "Expected Results: [1, 2, 2, 1, 2, 0, 0]\n",
      "\n",
      "Expected Results: ['versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version {0}\\n\" .format(tf.__version__))\n",
    "hidden_units = [10 , 30, 10]\n",
    "training_steps = 5000\n",
    "model_dir = \"/tmp/iris_model\"\n",
    "delete_old_model= True\n",
    "result = iris_classifier_dnn(hidden_units=hidden_units, \n",
    "                         training_steps=training_steps, \n",
    "                         model_dir=model_dir,\n",
    "                        delete_old_model=delete_old_model)\n",
    "print(\"Result: {0}\\n\" .format(result))\n",
    "print(\"Expected Results: [1, 2, 2, 1, 2, 0, 0]\\n\")\n",
    "print(\"Expected Results: ['versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa']\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logo.png\" style=\"height:75px;width:120px;\" align=\"left\">\n",
    "<ul>\n",
    "<li>You should get an accuracy of 96.6 in the above DNN for Iris </li>\n",
    "<li> You should get the following predictions:  [1, 2, 2, 1, 2, 0, 0]</li>\n",
    "<li>You should get the follwing predictions: ['versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa']</li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logo.png\" style=\"height:75px;width:120px;\" align=\"left\">\n",
    "\n",
    "<h1> Example Of SVM for the Same Iris Classification</h1>\n",
    "<ul>\n",
    "<li> Modify the code below, and add other SVM kernels, such as \"poly\", \"sigmoid\" , \"linear\" and \"rbf\"</li>\n",
    "<li> Modify the function iris_classifier_svm(kernel=\"poly\") to take in a parameter for kernel='all'\n",
    "<li> The function should return a JSON compaitable object (dictionary) that contains the results generated by each classifier\n",
    "<li> Print out results from 'all' SVM kernels for the iris example, and print their predictions for each kernel</li>\n",
    "<li> Add a decisionTree classified to the mix, and print out its prediction with the above 4  SVM kernels</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from six.moves.urllib.request import urlopen\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "label_name = 'Species'\n",
    "all_kernels=['linear', 'poly', 'rbf', 'sigmoid']\n",
    "def iris_classifier_svm(kernel='rbf',\n",
    "                    model_dir=\"/tmp/iris_model\",\n",
    "                    delete_old_model=False\n",
    "                   ):\n",
    "  \n",
    "  # 0 represents setosa 1 represents versicolor 2 represents virginica\n",
    "  iris_classes = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "    \n",
    "  # If the training and test sets aren't stored locally, download them.\n",
    "  if not os.path.exists(IRIS_TRAINING):\n",
    "    raw = urlopen(IRIS_TRAINING_URL).read()\n",
    "    with open(IRIS_TRAINING, \"wb\") as f:\n",
    "      f.write(raw)\n",
    "\n",
    "  if not os.path.exists(IRIS_TEST):\n",
    "    raw = urlopen(IRIS_TEST_URL).read()\n",
    "    with open(IRIS_TEST, \"wb\") as f:\n",
    "      f.write(raw)\n",
    "\n",
    "  if delete_old_model:\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "        print(\"Deleting old model {0}\\n\" .format(model_dir))\n",
    "  # Load datasets.\n",
    "  # Parse the local CSV file.\n",
    "  training_df = pd.read_csv(filepath_or_buffer=IRIS_TRAINING,\n",
    "                                names=CSV_COLUMN_NAMES,  # list of column names\n",
    "                                header=0  # ignore the first row of the CSV file.\n",
    "                               )\n",
    "    # train now holds a pandas DataFrame, which is data structure\n",
    "    # analogous to a table.\n",
    "\n",
    "    # 1. Assign the DataFrame's labels (the right-most column) to train_label.\n",
    "    # 2. Delete (pop) the labels from the DataFrame.\n",
    "    # 3. Assign the remainder of the DataFrame to train_features\n",
    "  train_features, train_label = training_df, training_df.pop(label_name)\n",
    "\n",
    "  \n",
    "  # Apply the preceding logic to the test set.\n",
    "  \n",
    "  test_df = pd.read_csv(IRIS_TEST, names=CSV_COLUMN_NAMES, header=0)\n",
    "  test_features, test_label = test_df, test_df.pop(label_name)\n",
    "\n",
    "\n",
    "  result={}\n",
    "\n",
    "  # Build SVM\n",
    "  if kernel=='all':\n",
    "    for k in range(len(all_kernels)):\n",
    "        kernel=all_kernels[k]\n",
    "        # Set the model type to be SVM-{kernel}\n",
    "        modelType = \"SVM-\" + kernel\n",
    "        print(\"Building SVM using a {0} Kernel\\n\" .format(kernel))\n",
    "\n",
    "        classifier = svm.SVC(decision_function_shape='ovo',kernel=kernel,  probability=True)\n",
    "        # Train model.\n",
    "        classifier.fit(train_features.values, train_label.values)\n",
    "          \n",
    "        # Evaluate accuracy.\n",
    "        accuracy_score_train = classifier.score(train_features.values, train_label.values)\n",
    "        print(\"\\nTrain Accuracy: {0:f}\".format(accuracy_score_train))\n",
    "        \n",
    "        # Test model\n",
    "        pred_labels = classifier.predict(test_features.values)\n",
    "        accuracy_score_test = accuracy_score(test_label, pred_labels)\n",
    "        print(\"Test Accuracy: {0:f}\\n\".format(accuracy_score_test))\n",
    "        # Classify two new flower samples.\n",
    "        # Classify two new flower samples.\n",
    "        new_samples = np.array(\n",
    "          [[6.4, 3.2, 4.5, 1.5],\n",
    "           [5.8, 3.1, 5.0, 1.7],\n",
    "           [6.4 ,2.8, 5.6 ,2.2],\n",
    "           [5.0 , 2.3 ,3.3 ,1.0 ],\n",
    "           [4.9 ,2.5 ,4.5 ,1.7 ],\n",
    "           [4.9 ,3.1 ,1.5 ,0.1 ],\n",
    "           [5.7,3.8 ,1.7 ,0.3]], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        predicted_classes = classifier.predict(new_samples)\n",
    "        probabilities = classifier.predict_log_proba(new_samples)\n",
    "        predicted_class_names = []\n",
    "        for i in range(len(predicted_classes)):\n",
    "            predicted_class_names.append(iris_classes[int(predicted_classes[i])])\n",
    "\n",
    "        #for i in range(len(predicted_classes)):\n",
    "        #  predicted_classes[i] = int(predicted_classes[i])\n",
    "        print(\n",
    "          \"New Samples, Class Predictions:    {}\\n\"\n",
    "          .format(predicted_classes))\n",
    "        print(\n",
    "          \"New Samples, Class Prediction Names:    {}\\n\"\n",
    "          .format(predicted_class_names))\n",
    "        print(\"----x----\")\n",
    "        result[all_kernels[k]]= {\"training-size\" :  train_features.shape,\n",
    "               \"test-size\" : test_features.shape,\n",
    "               \"Model-type\" : modelType,\n",
    "               \"model_dir\"  : model_dir,\n",
    "               \"accuracy\" : accuracy_score, \n",
    "               \"prediction\": predicted_classes,\n",
    "               \"prediction-names\": predicted_class_names}\n",
    "\n",
    "        \n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SVM using a linear Kernel\n",
      "\n",
      "\n",
      "Train Accuracy: 0.983333\n",
      "Test Accuracy: 0.966667\n",
      "\n",
      "New Samples, Class Predictions:    [1 2 2 1 2 0 0]\n",
      "\n",
      "New Samples, Class Prediction Names:    ['versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa']\n",
      "\n",
      "----x----\n",
      "Building SVM using a poly Kernel\n",
      "\n",
      "\n",
      "Train Accuracy: 0.991667\n",
      "Test Accuracy: 0.966667\n",
      "\n",
      "New Samples, Class Predictions:    [1 1 2 1 2 0 0]\n",
      "\n",
      "New Samples, Class Prediction Names:    ['versicolor', 'versicolor', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa']\n",
      "\n",
      "----x----\n",
      "Building SVM using a rbf Kernel\n",
      "\n",
      "\n",
      "Train Accuracy: 0.975000\n",
      "Test Accuracy: 1.000000\n",
      "\n",
      "New Samples, Class Predictions:    [1 2 2 1 2 0 0]\n",
      "\n",
      "New Samples, Class Prediction Names:    ['versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa']\n",
      "\n",
      "----x----\n",
      "Building SVM using a sigmoid Kernel\n",
      "\n",
      "\n",
      "Train Accuracy: 0.033333\n",
      "Test Accuracy: 0.033333\n",
      "\n",
      "New Samples, Class Predictions:    [0 0 0 2 2 2 0]\n",
      "\n",
      "New Samples, Class Prediction Names:    ['setosa', 'setosa', 'setosa', 'virginica', 'virginica', 'virginica', 'setosa']\n",
      "\n",
      "----x----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'linear': {'Model-type': 'SVM-linear',\n",
       "  'accuracy': <function sklearn.metrics.classification.accuracy_score>,\n",
       "  'model_dir': '/tmp/iris_model',\n",
       "  'prediction': array([1, 2, 2, 1, 2, 0, 0]),\n",
       "  'prediction-names': ['versicolor',\n",
       "   'virginica',\n",
       "   'virginica',\n",
       "   'versicolor',\n",
       "   'virginica',\n",
       "   'setosa',\n",
       "   'setosa'],\n",
       "  'test-size': (30, 4),\n",
       "  'training-size': (120, 4)},\n",
       " 'poly': {'Model-type': 'SVM-poly',\n",
       "  'accuracy': <function sklearn.metrics.classification.accuracy_score>,\n",
       "  'model_dir': '/tmp/iris_model',\n",
       "  'prediction': array([1, 1, 2, 1, 2, 0, 0]),\n",
       "  'prediction-names': ['versicolor',\n",
       "   'versicolor',\n",
       "   'virginica',\n",
       "   'versicolor',\n",
       "   'virginica',\n",
       "   'setosa',\n",
       "   'setosa'],\n",
       "  'test-size': (30, 4),\n",
       "  'training-size': (120, 4)},\n",
       " 'rbf': {'Model-type': 'SVM-rbf',\n",
       "  'accuracy': <function sklearn.metrics.classification.accuracy_score>,\n",
       "  'model_dir': '/tmp/iris_model',\n",
       "  'prediction': array([1, 2, 2, 1, 2, 0, 0]),\n",
       "  'prediction-names': ['versicolor',\n",
       "   'virginica',\n",
       "   'virginica',\n",
       "   'versicolor',\n",
       "   'virginica',\n",
       "   'setosa',\n",
       "   'setosa'],\n",
       "  'test-size': (30, 4),\n",
       "  'training-size': (120, 4)},\n",
       " 'sigmoid': {'Model-type': 'SVM-sigmoid',\n",
       "  'accuracy': <function sklearn.metrics.classification.accuracy_score>,\n",
       "  'model_dir': '/tmp/iris_model',\n",
       "  'prediction': array([0, 0, 0, 2, 2, 2, 0]),\n",
       "  'prediction-names': ['setosa',\n",
       "   'setosa',\n",
       "   'setosa',\n",
       "   'virginica',\n",
       "   'virginica',\n",
       "   'virginica',\n",
       "   'setosa'],\n",
       "  'test-size': (30, 4),\n",
       "  'training-size': (120, 4)}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class SVM\n",
    "# Other kernels = must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, \n",
    "kernel = \"all\"   \n",
    "iris_classifier_svm(kernel=kernel)\n",
    "\n",
    "## Modify code above to take in a parameter for kernel='all'\n",
    "## Print predictions from each SVM kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<li> You should get the following predictions:  [1, 2, 2, 1, 2, 0, 0]</li>\n",
    "<li>You should get the follwing predictions: ['versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa']</li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "**Linear** and **RBF** kernels give good results. However this function doesn't allow for tuning hyper parameters for regularization and uses the default values for poly, rbf and sigmoid. Since the new samples don't have labels, we cannot comment on whether the polynomial kernel is overfitting or giving the right results. SVM is very sensitive to feature scaling and it doesn't seem like that was done for the training data. The given code says test accuracy but it is done on the training set.So i took the liberty to evaluate the test accuracies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logo.png\" style=\"height:75px;width:120px;\" align=\"left\">\n",
    "<h2> Creating a Decision Tree classifie and printing all predictions </h2>\n",
    "<ul>\n",
    "<li> Wrote a function called iris_classifier_DT(kernel=\"all\") </li>\n",
    "<li> The function returns a JSON compatible object (dictionary) that contains all the results from each classifier\n",
    "<li> Print the Results object: Predictions from DecisionTree</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logo.png\" style=\"height:75px;width:120px;\" align=\"left\">\n",
    "<h2> Combine SVM and DNN iris_classifiers into a single wrapper and print all predictions </h2>\n",
    "<ul>\n",
    "<li> Write a  function called iris_clasifier() that calls <em>iris_classifier_svm(kernel=\"all\")<em> and <em>iris_classifier_dnn(..)</em></li>\n",
    "<li> The function should return a JSON compaitable object (dictionary) that contains all the results from each classifier\n",
    "<li> Print the Results object: Predictions from all SMV kernels, DecisionTree and DNN </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
